# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PK150rQTH97cRFNtIlKBcWJUBSKbQvP0
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
# !pip install transformers
from transformers import BertTokenizer, BertModel

from google.colab import drive
drive.mount('/content/drive')
data_dir = 'drive/My Drive/Hack the Mist/scores.csv'

data = pd.read_csv(data_dir)

# Convert the binary columns into strings
def binary_columns_to_text(row, column_names):
    text = ' '.join([col for col, value in zip(column_names, row) if value == 1])
    return text

# Convert binary columns to text
column_names = data.columns[:-1]
data["text"] = data.apply(lambda row: binary_columns_to_text(row[:-1], column_names), axis=1)

x = data["text"].values
y = data["score"].values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create a custom dataset class
class SustainabilityDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "label": torch.tensor(label, dtype=torch.float32),
        }

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
max_length = 120

train_dataset = SustainabilityDataset(x_train, y_train, tokenizer, max_length)
test_dataset = SustainabilityDataset(x_test, y_test, tokenizer, max_length)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Using BERT so our models better
class BERTRegressor(nn.Module):
    def __init__(self):
        super(BERTRegressor, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.fc = nn.Linear(self.bert.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        cls_token = last_hidden_state[:, 0, :]
        score = self.fc(cls_token)
        return score.squeeze()

model = BERTRegressor()

loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=2e-5)

num_epochs = 3

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(num_epochs):
    model.train()
    train_losses = []
    
    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        targets = batch["label"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, targets)

        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    train_loss = sum(train_losses) / len(train_losses)
    print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")

model.eval()

test_losses = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        targets = batch["label"].to(device)

        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, targets)
        test_losses.append(loss.item())

test_loss = sum(test_losses) / len(test_losses)
print(f"Test Loss: {test_loss:.4f}")

# Define a function to encode the inputs to use the model
def predict(model, tokenizer, texts, max_length=128, device="cpu"):
    model.eval()
    model.to(device)

    # Tokenize the input texts
    encoded_texts = tokenizer.batch_encode_plus(
        texts,
        add_special_tokens=True,
        max_length=max_length,
        return_token_type_ids=False,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt",
    )

    input_ids = encoded_texts["input_ids"].to(device)
    attention_mask = encoded_texts["attention_mask"].to(device)

    # Make predictions
    with torch.no_grad():
        predictions = model(input_ids, attention_mask)
    
    # Always use cpu
    return predictions.cpu().numpy()

# Example input texts (replace these with your own texts)
texts = ["adidas reebok", "calvin klein organic cotton", "nike fairtrade"]

# Make predictions
predictions = predict(model, tokenizer, texts)

# Print predictions
for text, prediction in zip(texts, predictions):
    print(f"Text: {text}, Score: {prediction:.4f}")

# Save the model and tokenizer for external use

# Save the model's state_dict (weights)
torch.save(model.state_dict(), "drive/My Drive/Hack the Mist/prediction_model_weights.pt")

# Save the tokenizer's configuration and vocabulary
tokenizer.save_pretrained("drive/My Drive/Hack the Mist/tokenizer")